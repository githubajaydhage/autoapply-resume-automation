name: Job Application System (Email Outreach)

# WORKING SYSTEM - Uses public job scraping + cold email outreach
# Browser automation is DISABLED (LinkedIn/Naukri/Indeed block it)

on:
  workflow_dispatch:
    inputs:
      job_location:
        description: 'Job Location'
        required: true
        default: 'Bangalore'
        type: choice
        options:
        - Bangalore
        - Mumbai
        - Delhi
        - Hyderabad
        - Pune
        - Remote
      max_emails:
        description: 'Max emails to send'
        required: true
        default: '10'
        type: choice
        options:
        - '5'
        - '10'
        - '20'
        - '50'
      scrape_only:
        description: 'Only scrape (no emails)'
        required: false
        default: 'false'
        type: choice
        options:
        - 'false'
        - 'true'
  schedule:
    - cron: '0 9 * * 1-5'  # 9 AM UTC weekdays (2:30 PM IST)

jobs:
  apply:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Restore Previous Data
      uses: actions/cache@v3
      with:
        path: |
          data/sent_emails_log.csv
          data/all_hr_emails.csv
          data/jobs_today.csv
        key: job-data-${{ runner.os }}-v2
        restore-keys: |
          job-data-${{ runner.os }}-

    - name: Step 1 - Scrape Jobs from Public Pages
      run: |
        echo "üîç PHASE 1: Scraping jobs from public career pages"
        echo "   LinkedIn, Naukri public listings (no login needed)"
        python scripts/linkedin_public_scraper.py
      env:
        PYTHONPATH: ${{ github.workspace }}
        JOB_LOCATION: ${{ github.event.inputs.job_location || 'Bangalore' }}

    - name: Step 2 - Scrape HR Emails
      run: |
        echo "üìß PHASE 2: Scraping HR/recruiter emails from job postings"
        python scripts/email_scraper.py
      env:
        PYTHONPATH: ${{ github.workspace }}

    - name: Step 3 - Send Application Emails
      if: ${{ github.event.inputs.scrape_only != 'true' }}
      run: |
        echo "üì§ PHASE 3: Sending personalized application emails"
        python scripts/email_sender.py
      env:
        PYTHONPATH: ${{ github.workspace }}
        SENDER_PASSWORD: ${{ secrets.SENDER_PASSWORD }}
        MAX_EMAILS: ${{ github.event.inputs.max_emails || '10' }}

    - name: Save Data Cache
      uses: actions/cache/save@v3
      with:
        path: |
          data/sent_emails_log.csv
          data/all_hr_emails.csv
          data/jobs_today.csv
        key: job-data-${{ runner.os }}-v2-${{ github.run_number }}

    - name: Upload Logs
      uses: actions/upload-artifact@v4
      with:
        name: application-logs
        path: |
          data/jobs_today.csv
          data/all_hr_emails.csv
          data/sent_emails_log.csv
          data/*.log
          data/*.json
      if: always()

    - name: Summary
      run: |
        echo "üìä ============================================"
        echo "üìä JOB APPLICATION SUMMARY"
        echo "üìä ============================================"
        if [ -f data/jobs_today.csv ]; then
          JOBS=$(wc -l < data/jobs_today.csv)
          echo "   Jobs scraped: $((JOBS - 1))"
        fi
        if [ -f data/all_hr_emails.csv ]; then
          EMAILS=$(wc -l < data/all_hr_emails.csv)
          echo "   HR emails found: $((EMAILS - 1))"
        fi
        if [ -f data/sent_emails_log.csv ]; then
          SENT=$(wc -l < data/sent_emails_log.csv)
          echo "   Emails sent (total): $((SENT - 1))"
        fi
        echo "üìä ============================================"
